# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9g71omwdBgc2bLHcifhVZUTk5d_H9zl
"""

import numpy as np

class Perceptron:
    def __init__(self, num_inputs, learning_rate=0.01, epochs=100):
        # Initialize the perceptron with random weights
        self.weights = np.random.rand(num_inputs)
        self.threshold = 0
        self.learning_rate = learning_rate
        self.epochs = epochs

    def activation_function(self, x):
        # Activation function is a step function
        return 1 if x > self.threshold else 0

    def train(self, X_train, y_train):
        # Training the perceptron using the perceptron learning rule
        for _ in range(self.epochs):
            for inputs, label in zip(X_train, y_train):
                prediction = self.predict(inputs)
                error = label - prediction
                self.weights += self.learning_rate * error * inputs

    def predict(self, inputs):
        # Compute the weighted sum
        weighted_sum = np.dot(inputs, self.weights)
        # Apply the activation function
        return self.activation_function(weighted_sum)

# Sample training data for the AND gate
X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_train = np.array([0, 0, 0, 1])

# Create and train the perceptron
perceptron = Perceptron(num_inputs=2)
perceptron.train(X_train, y_train)

# Test the perceptron
test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
for inputs in test_inputs:
    # Predict the output for each test input
    prediction = perceptron.predict(inputs)
    print(f"Input: {inputs}, Predicted Output: {prediction}")